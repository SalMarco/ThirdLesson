{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Introduction to Python\n",
    "================================\n",
    "\n",
    "Lesson 3 - Part1\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": true,
    "hidePrompt": false
   },
   "source": [
    "## Summary \n",
    "\n",
    "In this lesson we will focus on Neural Network. The lesson is divided between an introduction an a technical part\n",
    "The topis that we'll cover are in the introduction are:\n",
    "\n",
    "  - Tensorflow\n",
    "  - Keras \n",
    "  \n",
    "For the technical part we will create some models for:\n",
    "\n",
    "  - Single class classification\n",
    "  - Multi class classification\n",
    "  - Custom model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Today there are many framework that allow to create a Neural Network, like:\n",
    "\n",
    "  - Tensorflow \n",
    "  - Theano\n",
    "  - Caffe\n",
    "  - MXNett\n",
    "  - CNTK\n",
    "  \n",
    "Among all of them, probabilly the most famous is Tensorflow.\n",
    "\n",
    "Considering the support, the community the resources available, Tensorflow is the engine chosen for this course.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tensorflow\n",
    "\n",
    "[Tensorflow](https://www.tensorflow.org/) it was originally developed by Google, now it's open source.\n",
    "\n",
    "The definition from the web site is the following:\n",
    "\n",
    "*TensorFlow™ is an open source software library for high performance numerical computation. Its flexible architecture allows easy deployment of computation across a variety of platforms (CPUs, GPUs, TPUs), and from desktops to clusters of servers to mobile and edge devices. Originally developed by researchers and engineers from the Google Brain team within Google’s AI organization, it comes with strong support for machine learning and deep learning and the flexible numerical computation core is used across many other scientific domains.*\n",
    "\n",
    "Google has deeply invested in this project, to the point that now in Google Cloud there are available some instances with GPU's (called TPU's) designed especially for TensorFlow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Keras\n",
    "\n",
    "In order to simply the definition of the Neural Network and to speed up the development time we'll not use TensorFlow directly.\n",
    "Instead we'll use [Keras](https://keras.io/).\n",
    "\n",
    "The definition from the website is the following:\n",
    "\n",
    "*Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research.\n",
    "*\n",
    "\n",
    "As you can see from the description, Keras offers another advantage: with just a change of setting the same code can be used with:\n",
    "\n",
    "  - Tensorflow\n",
    "  - Theano\n",
    "  - CNTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison between Keras and Tensorflow\n",
    "\n",
    "In order to undestand the advantage of using Keras, we will consider the code necessary to define the same network using TensorFlow directly or Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## TensoFlow Multiclass\n",
    "\n",
    "Let's create a neural network to be used over the classic [mnsit](http://yann.lecun.com/exdb/mnist/) hand written digits with two hidden layer to classify samples in to the 10 possible classes. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss= 13092.2383, Training Accuracy= 0.305\n",
      "Step 2, Minibatch Loss= 9217.1924, Training Accuracy= 0.469\n",
      "Step 3, Minibatch Loss= 10747.7461, Training Accuracy= 0.359\n",
      "Step 4, Minibatch Loss= 8515.5430, Training Accuracy= 0.367\n",
      "Step 5, Minibatch Loss= 4846.1816, Training Accuracy= 0.578\n",
      "Step 6, Minibatch Loss= 4259.8452, Training Accuracy= 0.625\n",
      "Step 7, Minibatch Loss= 4784.9697, Training Accuracy= 0.578\n",
      "Step 8, Minibatch Loss= 3849.3728, Training Accuracy= 0.633\n",
      "Step 9, Minibatch Loss= 3358.5283, Training Accuracy= 0.656\n",
      "Step 10, Minibatch Loss= 1910.4451, Training Accuracy= 0.766\n",
      "Step 11, Minibatch Loss= 2261.0454, Training Accuracy= 0.688\n",
      "Step 12, Minibatch Loss= 1488.2395, Training Accuracy= 0.789\n",
      "Step 13, Minibatch Loss= 2552.8091, Training Accuracy= 0.734\n",
      "Step 14, Minibatch Loss= 3204.1516, Training Accuracy= 0.711\n",
      "Step 15, Minibatch Loss= 3431.4958, Training Accuracy= 0.719\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.7317\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "#mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.1\n",
    "num_steps = 15\n",
    "batch_size = 128\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of neurons\n",
    "n_hidden_2 = 256 # 2nd layer number of neurons\n",
    "num_input = 784 # MNIST data input (img shape: 28*28)\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([num_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n",
    "\n",
    "\n",
    "# Create model\n",
    "def neural_net(x):\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    # Output fully connected layer with a neuron for each class\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "# Construct model\n",
    "logits = neural_net(X)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(1, num_steps+1):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
    "                                                                 Y: batch_y})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for MNIST test images\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={X: mnist.test.images,\n",
    "                                      Y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-c3d55fec490c>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Keras Multiclass\n",
    "\n",
    "Let's create a neural network to be used over the classic [mnsit](http://yann.lecun.com/exdb/mnist/) hand written digits with two hidden layer to classify samples in to the 10 possible classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "10000/10000 [==============================] - 3s 262us/step - loss: 0.5023 - acc: 0.8582\n",
      "Epoch 2/15\n",
      "10000/10000 [==============================] - 1s 123us/step - loss: 0.1694 - acc: 0.9454\n",
      "Epoch 3/15\n",
      "10000/10000 [==============================] - 1s 115us/step - loss: 0.0975 - acc: 0.9697\n",
      "Epoch 4/15\n",
      "10000/10000 [==============================] - 1s 115us/step - loss: 0.0573 - acc: 0.9824\n",
      "Epoch 5/15\n",
      "10000/10000 [==============================] - 1s 118us/step - loss: 0.0372 - acc: 0.9877\n",
      "Epoch 6/15\n",
      "10000/10000 [==============================] - 1s 114us/step - loss: 0.0225 - acc: 0.9925\n",
      "Epoch 7/15\n",
      "10000/10000 [==============================] - 1s 115us/step - loss: 0.0096 - acc: 0.9977\n",
      "Epoch 8/15\n",
      "10000/10000 [==============================] - 1s 117us/step - loss: 0.0076 - acc: 0.9981\n",
      "Epoch 9/15\n",
      "10000/10000 [==============================] - 1s 119us/step - loss: 0.0268 - acc: 0.9914\n",
      "Epoch 10/15\n",
      "10000/10000 [==============================] - 1s 116us/step - loss: 0.0171 - acc: 0.9949\n",
      "Epoch 11/15\n",
      "10000/10000 [==============================] - 1s 118us/step - loss: 0.0083 - acc: 0.9971\n",
      "Epoch 12/15\n",
      "10000/10000 [==============================] - 1s 120us/step - loss: 0.0132 - acc: 0.9965\n",
      "Epoch 13/15\n",
      "10000/10000 [==============================] - 1s 134us/step - loss: 0.0072 - acc: 0.9981\n",
      "Epoch 14/15\n",
      "10000/10000 [==============================] - 1s 116us/step - loss: 0.0111 - acc: 0.9965\n",
      "Epoch 15/15\n",
      "10000/10000 [==============================] - 1s 140us/step - loss: 0.0016 - acc: 0.9998\n",
      "55000/55000 [==============================] - 7s 126us/step\n",
      "\n",
      "acc: 96.08%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import advanced_activations\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "num_steps = 15\n",
    "batch_size = 128\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of neurons\n",
    "n_hidden_2 = 256 # 2nd layer number of neurons\n",
    "num_input = 784 # MNIST data input (img shape: 28*28)\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "#\n",
    "activationFun = 'relu'\n",
    "#activationFun = 'softmax'\n",
    "\n",
    "#Definition of the model type\n",
    "model = Sequential()\n",
    "#Definition of the layers\n",
    "model.add(Dense(num_input, input_dim=num_input,activation=activationFun))\n",
    "model.add(Dense(n_hidden_1,activation=activationFun))\n",
    "model.add(Dense(n_hidden_2,activation=activationFun))\n",
    "model.add(Dense(num_classes,activation='softmax'))\n",
    "\n",
    "#Creation of the model\n",
    "adam = Adam(lr=learning_rate)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam,metrics=['accuracy'])\n",
    "#Fit of the network\n",
    "X = mnist.test.images\n",
    "Y = mnist.test.labels\n",
    "X_test = mnist.train.images\n",
    "Y_test = mnist.train.labels\n",
    "history = model.fit(X, Y, epochs=num_steps, batch_size=batch_size,validation_data=(X_test,Y_test))#validation_split=0.05\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, Y_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Warning\n",
    "\n",
    "Keras is an API, so the implementation of function is not one to one with TensorFlow. Often similar parameters can bring to different results.\n",
    "\n",
    "**You have to optimize the network for Keras, not for Tensorflow**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Results Analysis\n",
    "\n",
    "If we take a closer look to the results of the network what we can see?\n",
    "\n",
    "During the fit of the model, with `history = model.fit`, we have stored in `history` all the values of **loss** and **accuracy** for train and test.\n",
    "\n",
    "Note that must pass some validation data or a validation split using `validation_data` or `validation_split` parameters in `model.fit` \n",
    "\n",
    "We can define these and other metrics to be use using the parameter `metrics` in `model.compile`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9834503e29244f39fc0c0f152dbdfe1",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'val_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-24fe32513cc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mhistory_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mloss_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mval_loss_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_values\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bo'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'val_loss'"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "history_dict=history.history\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, 'bo')\n",
    "plt.plot(epochs, val_loss_values, 'b+')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Accuracy')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_values = history_dict['acc']\n",
    "val_loss_values = history_dict['val_acc']\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, 'bo')\n",
    "plt.plot(epochs, val_loss_values, 'b+')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Keras Single Class\n",
    "\n",
    "Now we'll make an example of a binary classification task using Keras.\n",
    "\n",
    "The dataset is hosted on [Kaggle](https://www.kaggle.com/) and it's the [Promotion Response for a New Product](https://www.kaggle.com/regivm/promotion-response-and-target-datasets/version/1).\n",
    "\n",
    "The description of th dataset is the following:\n",
    "\n",
    "*The context of this business problem is new product introduction. A business organization developed a new product and promoted this to its existing customers. Initially it chose a sample of customers for promotion and the response information is available in the 'promoted' dataset. The organization is interested in building a model to select the best customers for contacting from the pool of customers not contacted ('target' dataset).*\n",
    "\n",
    "and the columns are:\n",
    "\n",
    "  - customer_id\n",
    "  - **res** (what we want to predict)\n",
    "  - card_tenure\n",
    "  - risk_score\n",
    "  - num_promoted\n",
    "  - avg_bal\n",
    "  - geo_group\n",
    "  - res_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-07 20:17:47,684 : INFO : INIZIALIZATION OF LOADDATA\n",
      "2018-10-07 20:17:47,685 : INFO : READING data/target.csv\n",
      "2018-10-07 20:17:47,854 : INFO : LOADED DATASET WITH SHAPE (110000, 8) AND COLUMUNS Index(['customer_id', 'card_tenure', 'risk_score', 'num_promoted', 'avg_bal',\n",
      "       'geo_group', 'res_type', 'Unnamed: 7'],\n",
      "      dtype='object')\n",
      "2018-10-07 20:17:47,889 : INFO : READING data/promoted.csv\n",
      "2018-10-07 20:17:47,928 : INFO : LOADED DATASET WITH SHAPE (25000, 8) AND COLUMUNS Index(['customer_id', 'resp', 'card_tenure', 'risk_score', 'num_promoted',\n",
      "       'avg_bal', 'geo_group', 'res_type'],\n",
      "      dtype='object')\n",
      "2018-10-07 20:17:47,946 : INFO : REMOVING ROWS WITH NA\n",
      "2018-10-07 20:17:47,948 : INFO : NROWS BEFORE REMOVING NA 25000\n",
      "2018-10-07 20:17:47,959 : INFO : NROWS AFTER REMOVING NA 22400\n",
      "2018-10-07 20:17:47,962 : INFO : SCALING OF NUMERIC COLUMNS\n",
      "2018-10-07 20:17:47,983 : INFO : CONSIDERING LEVELS FOR CATEGORICAL COLUMNS\n",
      "2018-10-07 20:17:47,988 : INFO : INIZIALIZATION OF CreateNN\n",
      "2018-10-07 20:17:47,988 : INFO : DEFINITION OF THE MODEL\n",
      "2018-10-07 20:17:48,044 : INFO : COMPILATION OF THE MODEL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After reading          card_tenure     risk_score   num_promoted  Unnamed: 7\n",
      "count  107792.000000  110000.000000  110000.000000         0.0\n",
      "mean      138.956564     655.571482       0.006782         NaN\n",
      "std        67.433081      81.252328       0.082183         NaN\n",
      "min        12.000000     520.000000       0.000000         NaN\n",
      "25%        91.000000     600.000000       0.000000         NaN\n",
      "50%       135.000000     678.000000       0.000000         NaN\n",
      "75%       179.000000     720.000000       0.000000         NaN\n",
      "max       641.000000     760.000000       2.000000         NaN\n",
      "After reading                resp   card_tenure    risk_score  num_promoted\n",
      "count  25000.000000  24515.000000  25000.000000  25000.000000\n",
      "mean       0.068640    139.491617    655.091680      0.007000\n",
      "std        0.252846     66.998010     81.315116      0.083374\n",
      "min        0.000000      0.000000    520.000000      0.000000\n",
      "25%        0.000000     95.000000    599.000000      0.000000\n",
      "50%        0.000000    135.000000    677.000000      0.000000\n",
      "75%        0.000000    179.000000    719.000000      0.000000\n",
      "max        1.000000    641.000000    760.000000      1.000000\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_137 (Dense)            (None, 6)                 42        \n",
      "_________________________________________________________________\n",
      "dense_138 (Dense)            (None, 25)                175       \n",
      "_________________________________________________________________\n",
      "dense_139 (Dense)            (None, 10)                260       \n",
      "_________________________________________________________________\n",
      "dense_140 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 488\n",
      "Trainable params: 488\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-07 20:17:48,100 : INFO : EVALUATION OF THE MODEL\n",
      "2018-10-07 20:17:48,101 : INFO : START OF THE CROSS VALIDATION\n",
      "2018-10-07 20:17:48,105 : INFO : WORKING ON FOLD 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set [ 4481  4482  4483 ... 22397 22398 22399]\n",
      "Epoch 1/15\n",
      "17919/17919 [==============================] - 1s 73us/step - loss: 0.7491 - acc: 0.9126: 0s - loss: 0.9311 - acc: 0.90\n",
      "Epoch 2/15\n",
      "17919/17919 [==============================] - 0s 17us/step - loss: 0.2946 - acc: 0.9315\n",
      "Epoch 3/15\n",
      "17919/17919 [==============================] - 0s 18us/step - loss: 0.2807 - acc: 0.9315\n",
      "Epoch 4/15\n",
      "17919/17919 [==============================] - 0s 16us/step - loss: 0.2754 - acc: 0.9315\n",
      "Epoch 5/15\n",
      "17919/17919 [==============================] - 0s 14us/step - loss: 0.2657 - acc: 0.9315\n",
      "Epoch 6/15\n",
      "17919/17919 [==============================] - 0s 14us/step - loss: 0.2648 - acc: 0.9315\n",
      "Epoch 7/15\n",
      "17919/17919 [==============================] - 0s 15us/step - loss: 0.2625 - acc: 0.9315\n",
      "Epoch 8/15\n",
      "17919/17919 [==============================] - 0s 15us/step - loss: 0.2719 - acc: 0.9315\n",
      "Epoch 9/15\n",
      "17919/17919 [==============================] - 0s 18us/step - loss: 0.2590 - acc: 0.9315\n",
      "Epoch 10/15\n",
      "17919/17919 [==============================] - 0s 16us/step - loss: 0.2624 - acc: 0.9315\n",
      "Epoch 11/15\n",
      "17919/17919 [==============================] - 0s 15us/step - loss: 0.2601 - acc: 0.9315\n",
      "Epoch 12/15\n",
      "17919/17919 [==============================] - 0s 16us/step - loss: 0.2594 - acc: 0.9315\n",
      "Epoch 13/15\n",
      "17919/17919 [==============================] - 0s 16us/step - loss: 0.2540 - acc: 0.9315\n",
      "Epoch 14/15\n",
      "17919/17919 [==============================] - 0s 17us/step - loss: 0.2539 - acc: 0.9315\n",
      "Epoch 15/15\n",
      "17919/17919 [==============================] - 0s 18us/step - loss: 0.2624 - acc: 0.9315\n",
      "4481/4481 [==============================] - 0s 109us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-07 20:17:54,215 : INFO : WORKING ON FOLD 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set [    0     1     2 ... 22397 22398 22399]\n",
      "Epoch 1/15\n",
      "17920/17920 [==============================] - 0s 14us/step - loss: 0.2640 - acc: 0.9314\n",
      "Epoch 2/15\n",
      "17920/17920 [==============================] - 0s 17us/step - loss: 0.2604 - acc: 0.9314\n",
      "Epoch 3/15\n",
      "17920/17920 [==============================] - 0s 17us/step - loss: 0.2582 - acc: 0.9314\n",
      "Epoch 4/15\n",
      "17920/17920 [==============================] - 0s 14us/step - loss: 0.2527 - acc: 0.9314\n",
      "Epoch 5/15\n",
      "17920/17920 [==============================] - 0s 15us/step - loss: 0.2529 - acc: 0.9314\n",
      "Epoch 6/15\n",
      "17920/17920 [==============================] - 0s 14us/step - loss: 0.2500 - acc: 0.9314\n",
      "Epoch 7/15\n",
      "17920/17920 [==============================] - 0s 16us/step - loss: 0.2508 - acc: 0.9314\n",
      "Epoch 8/15\n",
      "17920/17920 [==============================] - 0s 16us/step - loss: 0.2552 - acc: 0.9314\n",
      "Epoch 9/15\n",
      "17920/17920 [==============================] - 0s 16us/step - loss: 0.2512 - acc: 0.9314\n",
      "Epoch 10/15\n",
      "17920/17920 [==============================] - 0s 16us/step - loss: 0.2577 - acc: 0.9314\n",
      "Epoch 11/15\n",
      "17920/17920 [==============================] - 0s 18us/step - loss: 0.2511 - acc: 0.9314\n",
      "Epoch 12/15\n",
      "17920/17920 [==============================] - 0s 17us/step - loss: 0.2520 - acc: 0.9314\n",
      "Epoch 13/15\n",
      "17920/17920 [==============================] - 0s 14us/step - loss: 0.2485 - acc: 0.9314\n",
      "Epoch 14/15\n",
      "17920/17920 [==============================] - 0s 15us/step - loss: 0.2500 - acc: 0.9314\n",
      "Epoch 15/15\n",
      "17920/17920 [==============================] - 0s 16us/step - loss: 0.2487 - acc: 0.9314\n",
      "4480/4480 [==============================] - 0s 32us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-07 20:17:58,629 : INFO : WORKING ON FOLD 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set [    0     1     2 ... 22397 22398 22399]\n",
      "Epoch 1/15\n",
      "17920/17920 [==============================] - 0s 16us/step - loss: 0.2480 - acc: 0.9314\n",
      "Epoch 2/15\n",
      "17920/17920 [==============================] - 0s 17us/step - loss: 0.2493 - acc: 0.9314\n",
      "Epoch 3/15\n",
      "17920/17920 [==============================] - 0s 17us/step - loss: 0.2482 - acc: 0.9314\n",
      "Epoch 4/15\n",
      "17920/17920 [==============================] - 0s 14us/step - loss: 0.2491 - acc: 0.9314\n",
      "Epoch 5/15\n",
      "17920/17920 [==============================] - 0s 14us/step - loss: 0.2485 - acc: 0.9314\n",
      "Epoch 6/15\n",
      "17920/17920 [==============================] - 0s 15us/step - loss: 0.2490 - acc: 0.9314\n",
      "Epoch 7/15\n",
      "17920/17920 [==============================] - 0s 14us/step - loss: 0.2484 - acc: 0.9314\n",
      "Epoch 8/15\n",
      "17920/17920 [==============================] - 0s 14us/step - loss: 0.2488 - acc: 0.9314\n",
      "Epoch 9/15\n",
      "17920/17920 [==============================] - 0s 14us/step - loss: 0.2480 - acc: 0.9314\n",
      "Epoch 10/15\n",
      "17920/17920 [==============================] - 0s 17us/step - loss: 0.2478 - acc: 0.9314\n",
      "Epoch 11/15\n",
      "17920/17920 [==============================] - 0s 17us/step - loss: 0.2477 - acc: 0.9314\n",
      "Epoch 12/15\n",
      "17920/17920 [==============================] - 0s 16us/step - loss: 0.2481 - acc: 0.9314\n",
      "Epoch 13/15\n",
      "17920/17920 [==============================] - 0s 16us/step - loss: 0.2500 - acc: 0.9314\n",
      "Epoch 14/15\n",
      "17920/17920 [==============================] - 0s 18us/step - loss: 0.2481 - acc: 0.9314\n",
      "Epoch 15/15\n",
      "17920/17920 [==============================] - 0s 16us/step - loss: 0.2479 - acc: 0.9314\n",
      "4480/4480 [==============================] - 0s 29us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-07 20:18:02,960 : INFO : WORKING ON FOLD 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set [    0     1     2 ... 22397 22398 22399]\n",
      "Epoch 1/15\n",
      "17920/17920 [==============================] - 1s 34us/step - loss: 0.2479 - acc: 0.9314\n",
      "Epoch 2/15\n",
      "17920/17920 [==============================] - 1s 42us/step - loss: 0.2487 - acc: 0.9314\n",
      "Epoch 3/15\n",
      "17920/17920 [==============================] - 1s 39us/step - loss: 0.2488 - acc: 0.9314\n",
      "Epoch 4/15\n",
      "17920/17920 [==============================] - 1s 39us/step - loss: 0.2498 - acc: 0.9314\n",
      "Epoch 5/15\n",
      "17920/17920 [==============================] - 1s 43us/step - loss: 0.2483 - acc: 0.9314\n",
      "Epoch 6/15\n",
      "17920/17920 [==============================] - 1s 38us/step - loss: 0.2480 - acc: 0.9314\n",
      "Epoch 7/15\n",
      "17920/17920 [==============================] - 1s 52us/step - loss: 0.2489 - acc: 0.9314\n",
      "Epoch 8/15\n",
      "17920/17920 [==============================] - 1s 43us/step - loss: 0.2499 - acc: 0.9314\n",
      "Epoch 9/15\n",
      "17920/17920 [==============================] - 1s 43us/step - loss: 0.2482 - acc: 0.9314\n",
      "Epoch 10/15\n",
      "17920/17920 [==============================] - 1s 44us/step - loss: 0.2482 - acc: 0.9314\n",
      "Epoch 11/15\n",
      "17920/17920 [==============================] - 1s 38us/step - loss: 0.2475 - acc: 0.9314\n",
      "Epoch 12/15\n",
      "17920/17920 [==============================] - 1s 38us/step - loss: 0.2483 - acc: 0.9314\n",
      "Epoch 13/15\n",
      "17920/17920 [==============================] - 1s 46us/step - loss: 0.2484 - acc: 0.9314\n",
      "Epoch 14/15\n",
      "17920/17920 [==============================] - 1s 41us/step - loss: 0.2478 - acc: 0.9314\n",
      "Epoch 15/15\n",
      "17920/17920 [==============================] - 0s 15us/step - loss: 0.2473 - acc: 0.9314\n",
      "4480/4480 [==============================] - 0s 27us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-07 20:18:13,791 : INFO : WORKING ON FOLD 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set [    0     1     2 ... 18008 18031 18032]\n",
      "Epoch 1/15\n",
      "17921/17921 [==============================] - 0s 17us/step - loss: 0.2483 - acc: 0.9314\n",
      "Epoch 2/15\n",
      "17921/17921 [==============================] - 0s 15us/step - loss: 0.2491 - acc: 0.9314\n",
      "Epoch 3/15\n",
      "17921/17921 [==============================] - 0s 15us/step - loss: 0.2490 - acc: 0.9314\n",
      "Epoch 4/15\n",
      "17921/17921 [==============================] - 0s 16us/step - loss: 0.2493 - acc: 0.9314\n",
      "Epoch 5/15\n",
      "17921/17921 [==============================] - 0s 14us/step - loss: 0.2523 - acc: 0.9314\n",
      "Epoch 6/15\n",
      "17921/17921 [==============================] - 0s 14us/step - loss: 0.2499 - acc: 0.9314\n",
      "Epoch 7/15\n",
      "17921/17921 [==============================] - 0s 17us/step - loss: 0.2503 - acc: 0.9314\n",
      "Epoch 8/15\n",
      "17921/17921 [==============================] - 0s 14us/step - loss: 0.2499 - acc: 0.9314\n",
      "Epoch 9/15\n",
      "17921/17921 [==============================] - 0s 15us/step - loss: 0.2499 - acc: 0.9314\n",
      "Epoch 10/15\n",
      "17921/17921 [==============================] - 0s 16us/step - loss: 0.2499 - acc: 0.9314\n",
      "Epoch 11/15\n",
      "17921/17921 [==============================] - 0s 15us/step - loss: 0.2496 - acc: 0.9314\n",
      "Epoch 12/15\n",
      "17921/17921 [==============================] - 0s 15us/step - loss: 0.2503 - acc: 0.9314\n",
      "Epoch 13/15\n",
      "17921/17921 [==============================] - 0s 18us/step - loss: 0.2486 - acc: 0.9314\n",
      "Epoch 14/15\n",
      "17921/17921 [==============================] - 1s 37us/step - loss: 0.2489 - acc: 0.9314\n",
      "Epoch 15/15\n",
      "17921/17921 [==============================] - 1s 42us/step - loss: 0.2490 - acc: 0.9314\n",
      "4479/4479 [==============================] - 0s 84us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-07 20:18:19,194 : INFO : EVALUATION COMPLETED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "acc: 0.93%+/-0.00%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import advanced_activations\n",
    "from keras.optimizers import Adam\n",
    "from sklearn import preprocessing\n",
    "import logging\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger('')\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "num_steps = 15\n",
    "batch_size = 128\n",
    "n_fold = 5\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 25 # 1st layer number of neurons\n",
    "n_hidden_2 = 10 # 2nd layer number of neurons\n",
    "num_input = 6 # MNIST data input (img shape: 28*28)\n",
    "num_classes = 1 # MNIST total classes (0-9 digits)\n",
    "\n",
    "trainFile = 'promoted.csv'\n",
    "predFile = 'target.csv'\n",
    "\n",
    "#\n",
    "activationFun = 'relu'\n",
    "#activationFun = 'softmax'\n",
    "\n",
    "class LoadData:\n",
    "    def __init__(self,**kargs):\n",
    "        self.path = 'data'\n",
    "        self.trainFile = kargs['tr']\n",
    "        \n",
    "    def readFiles(self,fileName):\n",
    "        fullPath = os.path.join(self.path,fileName)\n",
    "        logger.info('READING %s',fullPath)\n",
    "        df = pd.read_csv(fullPath,sep=',',dtype={'avg_bal':'category', 'geo_group':'category', 'res_type':'category',})#dtype={'avg_bal':'category', 'geo_group':'category', 'res_type':'category',}\n",
    "        logger.info('LOADED DATASET WITH SHAPE %s AND COLUMUNS %s',str(df.shape),str(df.columns))\n",
    "        print('After reading',df.describe())\n",
    "        return df\n",
    "        \n",
    "    def prepareTrain(self):\n",
    "        dfTrain = self.readFiles(self.trainFile)\n",
    "        logger.info('REMOVING ROWS WITH NA')\n",
    "        logger.info('NROWS BEFORE REMOVING NA %i',dfTrain.shape[0])\n",
    "        dfTrain.dropna(inplace=True)\n",
    "        logger.info('NROWS AFTER REMOVING NA %i',dfTrain.shape[0])\n",
    "        X_train = dfTrain.drop(columns=['resp'])\n",
    "        Y_train = dfTrain.loc[:,'resp']\n",
    "        logger.info('SCALING OF NUMERIC COLUMNS')\n",
    "        mmscaler = preprocessing.MinMaxScaler()\n",
    "        X_train[['card_tenure', 'risk_score', 'num_promoted']] = mmscaler.fit_transform(X_train[['card_tenure', 'risk_score', 'num_promoted']])  \n",
    "        logger.info('CONSIDERING LEVELS FOR CATEGORICAL COLUMNS')\n",
    "        for curCol in ['avg_bal','geo_group', 'res_type']:\n",
    "            X_train[curCol] = dfTrain[curCol].cat.codes\n",
    "        return X_train, Y_train\n",
    "\n",
    "class CreateNN:\n",
    "    def __init__(self,**kargs):\n",
    "        self.X_train = kargs['xt']\n",
    "        self.Y_train = kargs['yt']\n",
    "        self.kFold = kargs['kf']\n",
    "        self.i = 1\n",
    "        \n",
    "    def modelDefinition(self):\n",
    "        logger.info('DEFINITION OF THE MODEL')\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(num_input, input_dim = num_input,activation=activationFun))\n",
    "        self.model.add(Dense(n_hidden_1,activation = activationFun))\n",
    "        self.model.add(Dense(n_hidden_2,activation = activationFun))\n",
    "        self.model.add(Dense(num_classes,activation = 'sigmoid'))\n",
    "        print(self.model.summary())\n",
    "    \n",
    "    def modelCompile(self):\n",
    "        logger.info('COMPILATION OF THE MODEL')\n",
    "        adam = Adam(lr = learning_rate)\n",
    "        self.model.compile(loss = 'binary_crossentropy', optimizer = adam,metrics = ['accuracy'])\n",
    "        \n",
    "    def modelEval(self):\n",
    "        logger.info('EVALUATION OF THE MODEL')\n",
    "        totalScores = list()\n",
    "        logger.info('START OF THE CROSS VALIDATION')\n",
    "        for train,test in self.kFold.split(self.X_train, self.Y_train):\n",
    "            logger.info('WORKING ON FOLD %i',self.i)\n",
    "            print('train set',train)\n",
    "            history = self.model.fit(self.X_train.iloc[train], self.Y_train.iloc[train],\n",
    "                                     epochs=num_steps, \n",
    "                                     batch_size = batch_size) #validation_data=(self.X_train.iloc[test], self.Y_train.iloc[test])\n",
    "            scores = self.model.evaluate(self.X_train.iloc[test], self.Y_train.iloc[test])\n",
    "            totalScores.append(scores[1])\n",
    "            self.i += 1\n",
    "        return history, self.model, totalScores\n",
    "    \n",
    "    def modelPredict(self):\n",
    "        self.model.predict(......)\n",
    "    \n",
    "    \n",
    "def main():\n",
    "    #Inizialization of the class LoadData \n",
    "    logger.info('INIZIALIZATION OF LOADDATA')\n",
    "    ld = LoadData(tr=trainFile)\n",
    "    df2Pred = ld.readFiles(predFile)\n",
    "    X_train, Y_train = ld.prepareTrain()\n",
    "    kfold = StratifiedKFold(n_splits=n_fold)\n",
    "    logger.info('INIZIALIZATION OF CreateNN')\n",
    "    cnn = CreateNN(xt=X_train,yt=Y_train,kf=kfold)\n",
    "    cnn.modelDefinition()\n",
    "    cnn.modelCompile()\n",
    "    history, model, totalScores = cnn.modelEval() \n",
    "    logger.info('EVALUATION COMPLETED')\n",
    "    logger.info(\"FOR THE ACTUAL MODEL THE RESULTS OF %s IS: %.2f%%+/-%.2f%%\" % (model.metrics_names[1], np.mean(totalScores),np.std(totalScores)))\n",
    "    return X_train,Y_train, history,totalScores\n",
    "    \n",
    "X_train,Y_train, history,totalScores = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Try to understand the best parameters for the above model using what we have learned in this lesson (hint: look if the model is overfitting).\n",
    "\n",
    "After the definition of a good model, try to make the prediction over the pred set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "5/5 [==============================] - 1s 133ms/step - loss: 3.1273 - mean_squared_error: 3.1273\n",
      "Epoch 2/30\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.1515 - mean_squared_error: 0.1515\n",
      "Epoch 3/30\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0134 - mean_squared_error: 0.0134\n",
      "Epoch 4/30\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0085 - mean_squared_error: 0.0085\n",
      "Epoch 5/30\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0084 - mean_squared_error: 0.0084\n",
      "Epoch 6/30\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0083 - mean_squared_error: 0.0083\n",
      "Epoch 7/30\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0080 - mean_squared_error: 0.0080\n",
      "Epoch 8/30\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0077 - mean_squared_error: 0.0077\n",
      "Epoch 9/30\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0074 - mean_squared_error: 0.0074\n",
      "Epoch 10/30\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0072 - mean_squared_error: 0.0072\n",
      "Epoch 11/30\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0069 - mean_squared_error: 0.0069\n",
      "Epoch 12/30\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0067 - mean_squared_error: 0.0067\n",
      "Epoch 13/30\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0064 - mean_squared_error: 0.0064\n",
      "Epoch 14/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0062 - mean_squared_error: 0.0062\n",
      "Epoch 15/30\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0060 - mean_squared_error: 0.0060\n",
      "Epoch 16/30\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0058 - mean_squared_error: 0.0058\n",
      "Epoch 17/30\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0056 - mean_squared_error: 0.0056\n",
      "Epoch 18/30\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0054 - mean_squared_error: 0.0054\n",
      "Epoch 19/30\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0052 - mean_squared_error: 0.0052\n",
      "Epoch 20/30\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0050 - mean_squared_error: 0.0050\n",
      "Epoch 21/30\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0048 - mean_squared_error: 0.0048\n",
      "Epoch 22/30\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0047 - mean_squared_error: 0.0047\n",
      "Epoch 23/30\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0045 - mean_squared_error: 0.0045\n",
      "Epoch 24/30\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0043 - mean_squared_error: 0.0043\n",
      "Epoch 25/30\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0042 - mean_squared_error: 0.0042\n",
      "Epoch 26/30\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0040 - mean_squared_error: 0.0040\n",
      "Epoch 27/30\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0039 - mean_squared_error: 0.0039\n",
      "Epoch 28/30\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0037 - mean_squared_error: 0.0037\n",
      "Epoch 29/30\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0036 - mean_squared_error: 0.0036\n",
      "Epoch 30/30\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0035 - mean_squared_error: 0.0035\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a39cbdcf8>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "inputs = Input(shape=(1,))\n",
    "preds = Dense(1,activation='linear')(inputs)\n",
    "x=np.array([1,2,3,4,5])\n",
    "y=x*2\n",
    "\n",
    "model = Model(inputs=inputs,outputs=preds)\n",
    "sgd=keras.optimizers.SGD()\n",
    "model.compile(optimizer=sgd ,loss='mse',metrics=['mse'])\n",
    "model.fit(x,y, batch_size=1, epochs=30, shuffle=False)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
